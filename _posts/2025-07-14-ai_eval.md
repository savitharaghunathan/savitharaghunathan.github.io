---
layout: post
title: "Evaluating Gen AI Applications: Comparing Ragas and DeepEval"
date: 2025-07-14 12:00:00
description: >
    This is a beginner-friendly blog post for anyone exploring tools to evaluate RAG pipelines, summarization systems, or general LLM-driven applications. Throughout this post, you’ll find practical explanations and examples comparing Ragas and DeepEval, focusing on how they handle metrics like faithfulness, contextual precision, and answer relevancy. If you’re new to these concepts or just starting to assess your LLM outputs, this guide will give you a clear foundation. For a deeper dive, including installation, advanced configurations, and evolving features, be sure to explore each project’s official website and GitHub documentation.
tags: LLM, AI Eval, GenAI
categories: GenAI evaluation
pretty_table: true
---

*This is a beginner-friendly blog post for anyone exploring tools to evaluate RAG pipelines, summarization systems, or general LLM-driven applications. Throughout this post, you’ll find practical explanations and examples comparing Ragas and DeepEval, focusing on how they handle metrics like faithfulness, contextual precision, and answer relevancy. If you’re new to these concepts or just starting to assess your LLM outputs, this guide will give you a clear foundation. For a deeper dive, including installation, advanced configurations, and evolving features, be sure to explore each project’s official website and GitHub documentation.*

---

## Why evaluate LLM outputs at all?

We’ve all been using ChatGPT, Gemini, or other LLMs to help with various tasks professionally and personally. When you integrate an LLM into your own applications, like a chatbot, summarizer, or multi-step agent, you can’t just rely on gut feel that it works as expected.

You need to systematically measure things like:

* **Faithfulness:** is it hallucinating, or grounded in the evidence you provided?
* **Contextual precision:** does it only pull what’s needed from your docs?
* **Answer relevancy:** is it even addressing the user’s question?

That’s where specialized LLM evaluation frameworks come in.

For this post, we’ll be considering two tools to assess exactly these qualities: [Ragas](https://docs.ragas.io/en/stable/) and [DeepEval](https://deepeval.com).

---

## Ragas vs DeepEval

### **Ragas**

[Ragas](https://github.com/explodinggradients/ragas) is a Python library designed mainly for evaluating **RAG (retrieval-augmented generation)** systems. It offers metrics like:

* `faithfulness`
* `context_precision`
* `context_recall`
* `answer_relevancy`

It uses LLM-as-a-judge under the hood, sending structured prompts to models like GPT-4.

### **DeepEval**

[DeepEval](https://github.com/confident-ai/deepeval) is a broader LLM quality framework. Think of it as **pytest for LLMs**, allowing you to:

* Create `LLMTestCase` objects with `input`, `actual_output`, `expected_output`, and `retrieval_context`.
* Use `.measure()` to get scores or `assert_test()` to enforce quality thresholds in CI/CD.

It supports metrics like:

* `FaithfulnessMetric`
* `ContextualPrecisionMetric`
* `AnswerRelevancyMetric`
* plus `GEval` for writing your own natural language correctness checks.


---

## Key Example Questions & Responses

Both tools were tested on the same four example scenarios, illustrating typical, incomplete, and incorrect uses of context.

| Example | Question                                                      | LLM Response                                                                                                                          |
| ------- | ------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |
| 1       | Why do leaves change color in the fall?                       | Leaves change color in autumn because shorter days and cooler temperatures cause chlorophyll to break down, revealing other pigments. |
| 2       | Why do we have seasons on Earth?                              | Seasons occur because the Earth's axis is tilted, which changes how sunlight hits different parts of the planet throughout the year.  |
| 3       | Why do we have seasons on Earth? *(limited context)*          | We have seasons because Earth moves around the Sun.                                                                                   |
| 4       | Why do leaves change color in the fall? *(unrelated context)* | Leaves change color because fish swim upstream in autumn.                                                                             |

---

## Metric Results Overview

Below are the aggregated scores each tool produced for the four examples. Higher numbers are better (max = 1.000).

### Ragas Scores

| Example | Faithfulness | Context Precision | Context Recall | Answer Relevancy |
| ------- | ------------ | ----------------- | -------------- | ---------------- |
| 1       | 1.000        | 1.000             | 1.000          | 0.987            |
| 2       | 1.000        | 1.000             | 1.000          | 0.970            |
| 3       | 1.000        | 0.000             | 0.000          | 0.974            |
| 4       | 0.000        | 0.000             | 0.000          | 0.000            |

### DeepEval Scores

| Example | Faithfulness | Context Precision | Answer Relevancy |
| ------- | ------------ | ----------------- | ---------------- |
| 1       | 1.000        | 1.000             | 1.000            |
| 2       | 1.000        | 1.000             | 1.000            |
| 3       | 1.000        | 1.000             | 1.000            |
| 4       | 0.000        | 1.000             | 0.000            |

*Note: DeepEval does not expose a ****`context_recall`**** metric by default.*

---

## Biases to watch out for

| Bias name                                     | What it means                                                                               |
| --------------------------------------------- | ------------------------------------------------------------------------------------------- |
| **First case bias**                           | The LLM judge may favor the first listed option.                                            |
| **Self-evaluation bias (LLM evaluator bias)** | Tends to prefer outputs matching its own typical style.                                     |
| **Alignment bias (style calibration bias)**   | Rewards verbose, hedged, balanced language matching its training.                           |


---

## Bottom line

Both Ragas and DeepEval give you a structured way to quantify LLM quality beyond “looks fine to me.”

* Use **Ragas** for classic RAG pipelines.
* Use **DeepEval** for flexible, test-driven checks with custom criteria.

---

## Conclusion

Evaluating LLM outputs systematically is critical for building reliable AI applications. Ragas and DeepEval both excel in different contexts—Ragas for deep RAG‑specific insights, and DeepEval for robust, test‑driven workflows. By understanding their strengths, pitfalls, and biases, you can choose the right tool (or combine both) to ensure your LLM‑powered systems are accurate, precise, and trustworthy.

## References & Resources

- Demo notebooks:  
  - [DeepEval examples](https://github.com/savitharaghunathan/ai-eval-notebooks-playpen/blob/main/deepEval/multiple_questions.ipynb)  
  - [Ragas examples](https://github.com/savitharaghunathan/ai-eval-notebooks-playpen/blob/main/ragas/multiple_questions.ipynb)  
- [Ragas docs](https://docs.ragas.io/en/stable/) 
- [DeepEval docs](https://deepeval.com)
- [AI Engineering book by Chip Huyen](https://www.oreilly.com/library/view/ai-engineering/9781098166298/)