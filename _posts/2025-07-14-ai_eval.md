---
layout: post
title: "Evaluating Gen AI Applications: Comparing Ragas and DeepEval"
date: 2025-07-14 12:00:00
description: >
    This is a beginner-friendly blog post for anyone exploring tools to evaluate RAG pipelines, summarization systems, or general LLM-driven applications. Throughout this post, you’ll find practical explanations and examples comparing Ragas and DeepEval, focusing on how they handle metrics like faithfulness, contextual precision, and answer relevancy. If you’re new to these concepts or just starting to assess your LLM outputs, this guide will give you a clear foundation. For a deeper dive, including installation, advanced configurations, and evolving features, be sure to explore each project’s official website and GitHub documentation.
tags: LLM, AI Eval, GenAI
categories: GenAI evaluation
pretty_table: true
---

*This is a beginner-friendly blog post for anyone exploring tools to evaluate RAG pipelines, summarization systems, or general LLM-driven applications. Throughout this post, you’ll find practical explanations and examples comparing Ragas and DeepEval, focusing on how they handle metrics like faithfulness, contextual precision, and answer relevancy. If you’re new to these concepts or just starting to assess your LLM outputs, this guide will give you a clear foundation. For a deeper dive, including installation, advanced configurations, and evolving features, be sure to explore each project’s official website and GitHub documentation.*

---

## Why evaluate LLM outputs at all?

We’ve all been using ChatGPT, Gemini, or other LLMs to help with various tasks professionally and personally. When you integrate an LLM into your own applications, like a chatbot, summarizer, or multi-step agent, you can’t just rely on gut feel that it works as expected.

You need to systematically measure things like:

* **Faithfulness:** is it hallucinating, or grounded in the evidence you provided?
* **Contextual precision:** does it only pull what’s needed from your docs?
* **Answer relevancy:** is it even addressing the user’s question?

That’s where specialized LLM evaluation frameworks come in.

For this post, we’ll be considering two tools to assess exactly these qualities: [Ragas](https://docs.ragas.io/en/stable/) and [DeepEval](https://deepeval.com).

---

## Ragas vs DeepEval

### **Ragas**

[Ragas](https://github.com/explodinggradients/ragas) is a Python library designed mainly for evaluating **RAG (retrieval-augmented generation)** systems. It offers metrics like:

* `faithfulness` - Measures if the generated answer is faithful to the provided context
* `context_precision` - Evaluates how much of the retrieved context is relevant to the question
* `context_recall` - Measures how much of the relevant context was actually retrieved
* `answer_relevancy` - Assesses if the answer is relevant to the user's question


```python
from ragas import evaluate
from ragas.metrics import faithfulness, context_precision, context_recall, answer_relevancy

# Evaluate 
results = evaluate(
    dataset=your_dataset,
    metrics=[faithfulness, context_precision, context_recall, answer_relevancy]
)
```

It uses LLM-as-a-judge under the hood, sending structured prompts to models like GPT-4, and supports multiple evaluation models including local models.

### **DeepEval**

[DeepEval](https://github.com/confident-ai/deepeval) is a broader LLM quality framework. Think of it as **pytest for LLMs**, allowing you to:

* Create `LLMTestCase` objects with `input`, `actual_output`, `expected_output`, and `retrieval_context`.
* Use `.measure()` to get scores or `assert_test()` to enforce quality thresholds in CI/CD.

It supports metrics like:
* `faithfulness` 
* `context_precision` 
* `context_recall` 
* `answer_relevancy`

```python
from deepeval import LLMTestCase
from deepeval.metrics import FaithfulnessMetric, ContextualPrecisionMetric, AnswerRelevancyMetric, ContextualRecallMetric

# Create test case
test_case = LLMTestCase(
    input="Your question",
    actual_output="LLM response",
    expected_output="Expected response",
    retrieval_context=["context1", "context2"]
)

# Measure individual metrics
faithfulness = FaithfulnessMetric().measure(test_case)
context_precision = ContextualPrecisionMetric().measure(test_case)
answer_relevancy = AnswerRelevancyMetric().measure(test_case)
contextual_recall = ContextualRecallMetric().measure(test_case)
```

**Note**: DeepEval also supports RAGAS metrics through their `RagasMetric` class, which provides four RAGAS metrics (answer relevancy, faithfulness, contextual precision, and contextual recall). 

---

## Key Example Questions & Responses

Both tools were tested on the same four example scenarios, illustrating typical, incomplete, and incorrect uses of context.

| Example | Question                                                      | LLM Response                                                                                                                          |
| ------- | ------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------- |
| 1       | Why do leaves change color in the fall?                       | Leaves change color in autumn because shorter days and cooler temperatures cause chlorophyll to break down, revealing other pigments. |
| 2       | Why do we have seasons on Earth?                              | Seasons occur because the Earth's axis is tilted, which changes how sunlight hits different parts of the planet throughout the year.  |
| 3       | Why do we have seasons on Earth? *(limited context)*          | We have seasons because Earth moves around the Sun.                                                                                   |
| 4       | Why do leaves change color in the fall? *(unrelated context)* | Leaves change color because fish swim upstream in autumn.                                                                             |

---

## Metric Results Overview

Below are the aggregated scores each tool produced for the four examples. Higher numbers are better (max = 1.000).

### Ragas Scores

| Example | Faithfulness | Context Precision | Context Recall | Answer Relevancy |
| ------- | ------------ | ----------------- | -------------- | ---------------- |
| 1       | 1.000        | 1.000             | 1.000          | 0.987            |
| 2       | 1.000        | 1.000             | 1.000          | 0.970            |
| 3       | 1.000        | 0.000             | 0.000          | 0.974            |
| 4       | 0.000        | 0.000             | 0.000          | 0.000            |

### DeepEval Scores

| Example | Faithfulness | Context Precision | Answer Relevancy | Contextual Recall |
| ------- | ------------ | ----------------- | ---------------- | ----------------- |
| 1       | 1.000        | 1.000             | 1.000            | 1.000             |
| 2       | 1.000        | 1.000             | 1.000            | 1.000             |
| 3       | 1.000        | 1.000             | 1.000            | 1.000             |
| 4       | 0.000        | 1.000             | 0.000            | 0.000             |

---

## Key Observations from Results

### Example 1 & 2: Perfect Responses
Both tools correctly identified these as high-quality responses with perfect scores across all metrics. This demonstrates that when the context is relevant and the response is accurate, both evaluation frameworks perform consistently.

### Example 3: Limited Context Response
This reveals a key difference between the tools:

**Ragas** 
- Context Precision: 0.000 
- Context Recall: 0.000 

**DeepEval** 
- Context Precision: 1.000 
- Contextual Recall: 1.000 

**Analysis**: Ragas appears more sensitive to context quality. The response "We have seasons because Earth moves around the Sun" is technically correct but doesn't utilize the specific context about axial tilt. Ragas penalized this, while DeepEval gave full credit.

### Example 4: Unrelated Context Response
Both tools correctly identified the faithfulness issue (0.000), but showed different interpretations of context precision:

**DeepEval**: Context Precision 1.000  
**Ragas**: Context Precision 0.000

**Analysis**:  DeepEval might be measuring "did the system use the provided context?" while Ragas measures "was the provided context relevant to the question?" When context is completely unrelated, Ragas treats it as having zero precision.


## Biases to watch out for

| Bias name                                     | What it means                                                                               |
| --------------------------------------------- | ------------------------------------------------------------------------------------------- |
| **First case bias**                           | The LLM judge may favor the first listed option.                                            |
| **Self-evaluation bias (LLM evaluator bias)** | Tends to prefer outputs matching its own typical style.                                     |
| **Alignment bias (style calibration bias)**   | Rewards verbose, hedged, balanced language matching its training.                           |


## Conclusion

Evaluating LLM outputs systematically is critical for building reliable AI applications. Ragas and DeepEval both excel in different contexts—Ragas for deep RAG‑specific insights, and DeepEval for robust, test‑driven workflows. By understanding their strengths, pitfalls, and biases, you can choose the right tool (or combine both) to ensure your LLM‑powered systems are accurate, precise, and trustworthy.

## References & Resources

- Demo notebooks:  
  - [DeepEval examples](https://github.com/savitharaghunathan/ai-eval-notebooks-playpen/blob/main/deepEval/multiple_questions.ipynb)  
  - [Ragas examples](https://github.com/savitharaghunathan/ai-eval-notebooks-playpen/blob/main/ragas/multiple_questions.ipynb)  
- [Ragas docs](https://docs.ragas.io/en/stable/) 
- [DeepEval docs](https://deepeval.com)
- [AI Engineering book by Chip Huyen](https://www.oreilly.com/library/view/ai-engineering/9781098166298/)